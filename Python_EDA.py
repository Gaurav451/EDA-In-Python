#!/usr/bin/env python
# coding: utf-8

# In[2]:


import itertools
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import date

# columns=['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'year', 'name']

sns.set() # Default settings for Seaborn


# ## Importing Data and EDA

# In[3]:


df = pd.read_csv("used_cars_data.csv")


# In[4]:


df.head(5)


# In[5]:


df


# In[6]:


df.tail()


# In[7]:


# Analysis on checking for duplicates
df.nunique()


# In[8]:


# Checking for null values
df.isnull().sum()


# In[9]:


# The above calculations can also be done percentage wise
(df.isnull().sum()/len(df)) * 100


# ## Data Cleaning / Reduction

# In[10]:


# In the dataset, we can see that we have a column called "S.No", which is not very useful, both for analysis and dependent and independent variables
df.drop(columns = ['S.No.'], axis = 1)


# ## Feature Engineering

# In[11]:


# If we take a close look on data, we will see that the data has a column called "Year", which is Manufactured year of the car
# But, if we have a column called 'car_age', that will be quite useful.

df['Car_Age'] = date.today().year - df['Year']


# In[12]:


df.head(5)


# In[13]:


# The column 'Name' is not very useful, but if we extract 'Model Name' and 'Brand Name' from the 'Name' column, that will be useful

df['Brand_Name'] = df.Name.str.split().str.get(0)
df['Model_Name'] = df.Name.str.split().str.get(1) + df.Name.str.split().str.get(2)


# In[14]:


df.head(5)


# In[15]:


df[['Location', 'Owner_Type', 'Engine', 'Price']]


# ## Data cleaning and Wrangling

# In[16]:


# In the 'Brand_Name' column, we can see that some entries have inconsistency among them. We need to fix it.

print(df.Brand_Name.unique())
print(df.Brand_Name.nunique())


# In[17]:


# Looking in the 'Brand_Name' column, we came to know that 'Isuzu', 'ISUZU', 'Mini', 'Land' are anomalies in the data

search_for = ['Isuzu', 'ISUZU', 'Mini', 'Land']

df[df.Brand_Name.str.contains('|'.join(search_for))]


# In[18]:


# Replacing the above anomalies

df['Brand_Name'].replace({"ISUZU" : 'Isuzu', "Mini" : "Mini Cooper", "Land" : "Land Rover"}, inplace = True)


# In[19]:


df


# ## EDA (Exploratory Data Analysis)

# In[20]:


# Getting a high level view of data

df.describe()


# In[21]:


df.describe(include = 'all')


# In[22]:


# Separating Categorical and Numerical Data

cols_data = df.select_dtypes(include = ['object']).columns
numeric_data = df.select_dtypes(include =np.number).columns.tolist()

print("Categorical data  ", cols_data)
print("Numerical data  ", numeric_data)


# ## Univariate Analysis

# In[23]:


# Now, we will perform Unviariate analysis separately on Numerical and Categorical data

for col in numeric_data:
    print(col)
    print('Skew :', round(df[col].skew(), 2))
    plt.figure(figsize = (15, 4))
    plt.subplot(1, 2, 1)
    df[col].hist(grid=False)
    plt.ylabel('count')
    plt.subplot(1, 2, 2)
    sns.boxplot(x=df[col])
    plt.show()


# In[24]:


fig, axes = plt.subplots(3, 2, figsize = (18, 18))
fig.suptitle('Bar plot for all categorical variables in the dataset')
sns.countplot(ax = axes[0, 0], x = 'Fuel_Type', data = df, color = 'blue', 
              order = df['Fuel_Type'].value_counts().index);
sns.countplot(ax = axes[0, 1], x = 'Transmission', data = df, color = 'blue', 
              order = df['Transmission'].value_counts().index);
sns.countplot(ax = axes[1, 0], x = 'Owner_Type', data = df, color = 'blue', 
              order = df['Owner_Type'].value_counts().index);
sns.countplot(ax = axes[1, 1], x = 'Location', data = df, color = 'blue', 
              order = df['Location'].value_counts().index);
sns.countplot(ax = axes[2, 0], x = 'Brand_Name', data = df, color = 'blue', 
              order = df['Brand_Name'].head(20).value_counts().index);
sns.countplot(ax = axes[2, 1], x = 'Model_Name', data = df, color = 'blue', 
              order = df['Model_Name'].head(20).value_counts().index);
axes[1][1].tick_params(labelrotation=45);
axes[2][0].tick_params(labelrotation=90);
axes[2][1].tick_params(labelrotation=90);


# ### A lot of insights can be generated by above graphs ->
# ### 1.) In the 'Owner_Type' chart, we can see that major sales of cars were done as First hand cars.
# ### 2.) Manual transmission cars are more soughted than Automatic cars, etc

# In[25]:


# Variables with higher skewness must be Log Transformed to come up to other variables.
# Function for Log Transformation

def Log_Transformation(data, cols):
    for colname in cols:
        if (df[colname] == 1.0).all():
            df[colname + '_log'] = np.log(df[colname] + 1)
        else :
            df[colname + '_log'] = np.log(df[colname])


# In[26]:


Log_Transformation(df, ['Kilometers_Driven', 'Price'])


# In[27]:


# Graphical representation of Log Transformation

sns.distplot(df['Kilometers_Driven_log'], axlabel = "Kilometers_Driven_log")


# In[28]:


sns.distplot(df['Price_log'], axlabel = "Price_log")


# ## EDA BiVariate Analysis

# In[31]:


# This analysis is mainly used to check the correlation between 2 or more variables. 
# This analysis is also useful if you have a classifier as your output.

plt.figure(figsize=(13,17))
sns.pairplot(data = df.drop(["Kilometers_Driven", "Price"], axis = 1))
plt.show()


# ## EDA Multivariate Analysis

# In[32]:


# Above pairplots can be difficult to understand for some people to understand.
# So, we can use a "Heat Map" to clearly show the correlation between the variables.

plt.figure(figsize=(12, 7))
sns.heatmap(data = df.drop(["Kilometers_Driven", "Price"], axis = 1).corr(), annot = True, vmin = -1, vmax = 1)
plt.show()


# #### From the above heatmap, we can infer the following ->
# #### 1.) Year and Price_log have a good correlation with them, infering that Year will put a good effect on Price_log
# #### 2.) Kilometers_Driven_log and Seats have a fair correlation with them, infering that more the car gets used, seats condition will get efffected.
# 
# #### etc....

# ## Imputing missing values

# In[ ]:


# Some columns, like mileage, seats, etc have missing values, like NA, etc.
# For imputing, we have some basic technniques
# We can fill missing values with the mean/median of the respective column, etc
# We can also use the domain knowledge to fill missing values too
# And lastly, some assumptions can also be made to fill the missing data


# In[42]:


# Filling missing values with Null

df.loc[df["Mileage"]==0.0,'Mileage']=np.nan
df.Mileage.isnull().sum()

df["Mileage"].fillna(value=np.nan,inplace=True)


# In[43]:


df.head(5)


# In[55]:


# Imputing seats, engine, power columns too

#df['Seats'] = df.groupby(['Brand_Name', 'Model_Name'])['Seats'].apply(lambda x : x.fillna(x.median()))
#df['Engine'] = df.groupby(['Brand_Name', 'Model_Name'])['Engine'].apply(lambda x : x.fillna(x.median()))
#df['Power'] = df.groupby(['Brand_Name', 'Model_Name'])['Power'].apply(lambda x : x.fillna(x.median()))

#df['Engine'] = pd.to_numeric(df['Prices'], errors='coerce')
df['Engine'] = pd.to_numeric(df['Engine'], errors = 'coerce')
df['Engine']=df.groupby(['Brand_Name','Model_Name'])['Engine'].apply(lambda x:x.fillna(x.median()))

df['Seats'] = pd.to_numeric(df['Seats'], errors = 'coerce')
df['Seats'] = df.groupby(['Brand_Name', 'Model_Name'])['Seats'].apply(lambda x:x.fillna(x.median()))

df['Power'] = pd.to_numeric(df['Power'], errors = 'coerce')
df['Power'] = df.groupby(['Brand_Name', 'Model_Name'])['Power'].apply(lambda x:x.fillna(x.median()))


# In[56]:


df


# # Thank You
